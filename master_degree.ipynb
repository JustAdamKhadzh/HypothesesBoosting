{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import imp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import functions as utils\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import partial\n",
    "from concurrent.futures import ProcessPoolExecutor, Executor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path, filename):\n",
    "    print(f'reading file = {os.path.join(path, filename)}')\n",
    "    data = pd.read_csv(os.path.join(path, filename))\n",
    "    data = data.rename(columns={'Unnamed: 0':'Id'})\n",
    "    print(f'data shape = {data.shape}')\n",
    "    types_info = pd.DataFrame(data.dtypes.value_counts(), columns=['columns_count'])\n",
    "    print('types info about df columns: ')\n",
    "    print(types_info)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls -la -h Datasets/GiveMeSomeCredit/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file = Datasets/GiveMeSomeCredit/cs-training.csv\n",
      "data shape = (150000, 12)\n",
      "types info about df columns: \n",
      "         columns_count\n",
      "int64                8\n",
      "float64              4\n"
     ]
    }
   ],
   "source": [
    "train_data = read_data(path='Datasets/GiveMeSomeCredit/', filename='cs-training.csv')\n",
    "# test_data = read_data(path='Datasets/GiveMeSomeCredit/', filename='cs-test.csv')\n",
    "# descript = pd.read_excel(\"Datasets/GiveMeSomeCredit/Data Dictionary.xls\")\n",
    "# sample_data = pd.read_csv(\"Datasets/GiveMeSomeCredit/sampleEntry.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117910, 12)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revolve_thresh = round(np.quantile(train_data.RevolvingUtilizationOfUnsecuredLines.values, q=[0.99])[0], 3)\n",
    "# debtratio_thresh = round(np.quantile(train_data.DebtRatio.values, q=[0.9])[0], 3)\n",
    "debtratio_thresh = 2.1\n",
    "revolve_thresh\n",
    "\n",
    "train_data = train_data[(train_data.RevolvingUtilizationOfUnsecuredLines < revolve_thresh) & \n",
    "                        (train_data.DebtRatio < debtratio_thresh)\n",
    "                       ]\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.fillna(int(train_data.MonthlyIncome.mean()), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.fillna(0, inplace=True)\n",
    "# test_data.fillna(0, inplace=True)\n",
    "\n",
    "#ставим колонку Id как индекс клиента\n",
    "train_data.set_index('Id', inplace=True)\n",
    "# test_data.set_index('Id', inplace=True)\n",
    "\n",
    "#сохраняем метку класса\n",
    "train_label = train_data['SeriousDlqin2yrs'].copy()\n",
    "train_data.drop('SeriousDlqin2yrs', axis=1, inplace=True)\n",
    "#удаляем колонку класса из тестовых данных, так как она не несет никакой информации\n",
    "# test_data.drop('SeriousDlqin2yrs', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['NumberOfDependents'] = train_data.NumberOfDependents.astype('int')\n",
    "train_data['MonthlyIncome'] = train_data.MonthlyIncome.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117910, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape#, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_cols = train_data.select_dtypes('float').columns\n",
    "train_data.loc[:, float_cols] = train_data.loc[:, float_cols].round(2)\n",
    "# train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# float_cols = test_data.select_dtypes('float').columns\n",
    "# test_data.loc[:, float_cols] = test_data.loc[:, float_cols].round(2)\n",
    "# test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train = utils.transform_to_description(train_data)\n",
    "# transformed_test = transform_to_description(test_data)\n",
    "# transformed_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117910, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_train.shape#, transformed_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(transformed_train, train_label, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "valX, testX, valY, testY = train_test_split(testX, testY, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70746, 10) (23582, 10) (23582, 10)\n",
      "(70746,) (23582,) (23582,)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape, valX.shape, testX.shape)\n",
    "print(trainY.shape, valY.shape, testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.93382\n",
       "1    0.06618\n",
       "Name: SeriousDlqin2yrs, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.934229\n",
       "1    0.065771\n",
       "Name: SeriousDlqin2yrs, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valY.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.932576\n",
       "1    0.067424\n",
       "Name: SeriousDlqin2yrs, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Алгоритм из работы Алексея(QBCA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = 0.005\n",
    "# sample_ratio = 0.01\n",
    "# num_iters = 100\n",
    "# N_neg = train_label.value_counts().reset_index().iloc[0, 1]\n",
    "# N_pos = train_label.value_counts().reset_index().iloc[1, 1]\n",
    "# N_neg, N_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**не обновляем индекс так как индекс - это id клиента, имеет значимую информацию**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((66064, 10), (4682, 10))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_pos = transformed_train.loc[train_label[train_label == 1].index]\n",
    "# train_neg = transformed_train.loc[train_label[train_label == 0].index]\n",
    "train_pos = trainX.loc[trainY[trainY == 1].index]\n",
    "train_neg = trainX.loc[trainY[trainY == 0].index]\n",
    "\n",
    "train_neg.shape, train_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.07087067086461613, 14.110209312259718)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_ratio_pos = train_pos.shape[0] / train_neg.shape[0]\n",
    "classes_ratio_neg = train_neg.shape[0] / train_pos.shape[0]\n",
    "classes_ratio_pos, classes_ratio_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Берем выборку заемов, для начального тестирования работоспособности**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 10), (2000, 10))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos = train_pos.sample(n=1000, replace=False)\n",
    "train_neg = train_neg.sample(n=1000, replace=False)\n",
    "train_pos.shape, train_neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4412, 10), (65621, 10))"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos.shape, train_neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_sample = valX.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mining step\n",
    "\n",
    "\n",
    "    для положительного класса нас интересуют объекты отрицательного класса, \n",
    "    а для отрицательного - положительные\n",
    "    Уже после определения объектов из другого класса, попадающие в признаковое представление семпла данных, будет приниматься решение\n",
    "     о включение этого признакого представление в список гипотез представления(областей или интервальных представлений)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что нужно посмотреть:\n",
    "\n",
    "    1. Сравнение количества генерируемых гипотез в зависимости от критерия и области генерации выборки.\n",
    "    (Возможно изобразить всего 4 варианта: старый подход - локальная или случайная выборка, новый подход - \n",
    "    локальная или случайная выборка)\n",
    "    \n",
    "    2. Сравнение генерируемых гипотез для старого и нового критерия в зависимости от ширины локальной области генерации\n",
    "    (Возможно есть какая то оптимальная ширина окна)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-do-list:\n",
    "1. **классификации при разном включении признаков из исходного множества**\n",
    "\n",
    "2. **Подумать о том, каким образом генерить гипотезы. То есть выбирать не случайно выборку из множества объектов,\n",
    "    а какую-то локальную область. (Делать будем через расширение области объекта; более того, будем искать \n",
    "    оптимальное значение расширения локальной области) done !!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = utils.find_opt_local_area(obj=test_sample.iloc[0],\n",
    "#                         train_data=train_pos,\n",
    "#                         trainx_min=trainx_min,\n",
    "#                         trainx_max=trainx_max,\n",
    "#                         frac=0.15,\n",
    "#                         num_iters=60\n",
    "#                        )\n",
    "\n",
    "# utils.generate_random_sample(train_data=train_pos, sample_size=10, d=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils = imp.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hypothesis(iteration: int, obj: pd.Series, object_area: pd.Series, train_data: pd.DataFrame, \n",
    "                        other_data: pd.DataFrame, sample_size: int, classes_ratio: float,\n",
    "                        hypothesis_criterion: str, sample_type:str, verbose: bool, alpha: float):\n",
    "        \n",
    "        print(f'iteration: {iteration}')\n",
    "        \n",
    "        if sample_type == 'local' and object_area is None:\n",
    "            print(f'Cannot generate sample from local area. Got None as local object area param!')\n",
    "            raise NotImplementedError('Wrong params for local sampling!')\n",
    "    \n",
    "        if sample_type == 'random' and object_area is not None:\n",
    "            print(f'got misleading params values. Got sample_type = None and local object area is not None')\n",
    "            return NotImplementedError('Wrong params for random sampling!')\n",
    "\n",
    "        #generating random sample using sample strategy = {sample_type}\n",
    "        sample = utils.generate_random_sample(train_data=train_data, sample_size=sample_size, d=object_area) \n",
    "        sample.append(obj)\n",
    "        \n",
    "        d = utils.get_similarity_sample_repr(sample)\n",
    "        if verbose:\n",
    "            print('got feature represantation for sample')\n",
    "        \n",
    "        d_other_objects = utils.is_included_in_repr(d, train_data=other_data)\n",
    "        if d_other_objects is not None:\n",
    "            print(f'got {d_other_objects.shape[0]} d_other_objects')\n",
    "            print(f'thresh for hypothesis = {int(other_data.shape[0] * alpha)}')\n",
    "    \n",
    "        if d_other_objects is None:\n",
    "            return d    \n",
    "        \n",
    "        result_hypothesis = utils.check_criterion(d=d, train_data=train_data, \n",
    "                                                  hypothesis_criterion=hypothesis_criterion, \n",
    "                                                  d_other_objects=d_other_objects,\n",
    "                                                  other_data=other_data, alpha=alpha, \n",
    "                                                  classes_ratio=classes_ratio\n",
    "                                                 )\n",
    "\n",
    "        return result_hypothesis\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "def mining_step(test_obj: pd.Series, train_pos: pd.DataFrame, train_neg: pd.DataFrame, num_iters: int, \n",
    "                sample_ratio: float, alpha: float, hypothesis_criterion: str, sample_type: str,\n",
    "                trainx_min: pd.Series, trainx_max:pd.Series, classes_ratio: float, \n",
    "                mining_type: str = 'pos', verbose : bool = False, n_jobs : int = 4):\n",
    "    \"\"\"\n",
    "    hypothesis_criterion: 'contr_class', если используем базовый критерий, \n",
    "                                когда смотрится пересечение с противоположным классом(старый критерий отбора гипотез)\n",
    "                           'both_classes', когда интересует пересечение по обоим классам(новый критерий отбора гипотез)\n",
    "                           \n",
    "    sample_type: 'random', если берем произвольную выборку интервальных представлений\n",
    "                 'local', если берем произвольную выборку из локальной области\n",
    "    \n",
    "    returns list of hypothesises\n",
    "    \"\"\"\n",
    "    \n",
    "    if sample_type == 'local':\n",
    "        #sampling\n",
    "        print(f'start searching optimal local area')\n",
    "        object_area = utils.find_opt_local_area(obj=test_obj,\n",
    "                                                train_data=train_data,\n",
    "                                                trainx_min=trainx_min,\n",
    "                                                trainx_max=trainx_max,\n",
    "                                                frac=0.15,\n",
    "                                                num_iters=30\n",
    "                                               )\n",
    "    else:\n",
    "        print(f'using random sample from train data')\n",
    "        object_area = None\n",
    "    \n",
    "    train_data = train_pos if mining_type == 'pos' else train_neg\n",
    "    other_data = train_neg if mining_type == 'pos' else train_pos\n",
    "    sample_size = int(train_data.shape[0] * sample_ratio)\n",
    "    print('start generating hypothesises')\n",
    "    \n",
    "    mining = partial(generate_hypothesis, \n",
    "                     obj=test_obj, \n",
    "                     object_area=object_area,\n",
    "                     train_data=train_data, \n",
    "                     other_data=other_data, \n",
    "                     sample_size=sample_size,\n",
    "                     classes_ratio=classes_ratio,\n",
    "                     hypothesis_criterion=hypothesis_criterion,\n",
    "                     sample_type=sample_type,\n",
    "                     verbose=verbose,\n",
    "                     alpha=alpha\n",
    "                    )\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=n_jobs) as executor:\n",
    "        hypothesises = executor.map(mining, range(num_iters))\n",
    "\n",
    "    hypothesises = [res for res in hypothesises if res is not None]\n",
    "    print(f'All hypothesises are generated!')\n",
    "    return hypothesises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pos = {(0.0, 0.001, 1000): 1000,\n",
    " (0.0, 0.002, 1000): 608,\n",
    " (0.0, 0.004, 1000): 17,\n",
    " (0.001, 0.001, 1000): 1000,\n",
    " (0.001, 0.002, 1000): 740,\n",
    " (0.001, 0.004, 1000): 37,\n",
    " (0.002, 0.001, 1000): 1000,\n",
    " (0.002, 0.002, 1000): 790,\n",
    " (0.002, 0.004, 1000): 51}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Генерация гипотез для полож класса занимает 10 мин (с 4 процессами) c 3000 iterations**\n",
    "**c 1000 iterations - 2 mins**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for i, obj in test_sample.iterrows():\n",
    "#     print(type(obj))\n",
    "#     print(f'start mining from pos objects')\n",
    "#     pos_hyps = mining_step(test_obj=obj, train_pos=train_pos, train_neg=train_neg, \n",
    "#                            num_iters=num_iters,sample_ratio=sample_ratio, alpha = alpha,\n",
    "#                            hypothesis_criterion='contr_class',\n",
    "#                            sample_type='random',\n",
    "#                            mining_type='pos',\n",
    "#                            verbose=False, n_jobs=4\n",
    "#                           )\n",
    "    \n",
    "#     print(f'start mining from neg objects')\n",
    "#     neg_hyps = mining_step(test_obj=obj, train_pos=train_pos, train_neg=train_neg,\n",
    "#                            num_iters=num_iters, sample_ratio=sample_ratio, alpha = alpha, \n",
    "#                            hypothesis_criterion='contr_class',mining_type='neg', \n",
    "#                            sample_type='random',\n",
    "#                            verbose=False, n_jobs=4\n",
    "#                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_hyps = pd.DataFrame(pos_hyps)\n",
    "neg_hyps = pd.DataFrame(neg_hyps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_hyps = pd.DataFrame(pos_hyps)\n",
    "neg_hyps = pd.DataFrame(neg_hyps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((761, 10), (789, 10))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_hyps.shape, neg_hyps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg_hyps.to_csv('hypothesises/test_neg_hyps.csv', header=True, index=False)\n",
    "# pos_hyps = pd.read_csv('hypothesises/test_pos_hyps.csv')\n",
    "# neg_hyps = pd.read_csv('hypothesises/test_neg_hyps.csv')\n",
    "# pos_hyps.shape, neg_hyps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils = imp.reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Исследования количества итераций, необходимых для достижения определенной доли объектов обучающей выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.generate_local_area(test_sample.iloc[0], trainx_min, trainx_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 10), (1000, 10))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos = train_pos.sample(n=1000, replace=False)\n",
    "train_neg = train_neg.sample(n=1000, replace=False)\n",
    "train_pos.shape, train_neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx_min = train_data.min()\n",
    "trainx_max = train_data.max()\n",
    "test_sample = valX.sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01, 0.03, 0.05, 0.07, 0.09, 0.11, 0.13, 0.15, 0.17, 0.19, 0.21]\n"
     ]
    }
   ],
   "source": [
    "fractions = list(range(1, 23, 2))\n",
    "fractions = [x/100 for x in fractions]\n",
    "num_iterations = 1000\n",
    "print(fractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterations_list_pos = {}\n",
    "iterations_list_neg = {}\n",
    "\n",
    "for frac in fractions:\n",
    "    iterations_list_pos[frac] = []\n",
    "    iterations_list_neg[frac] = []\n",
    "num_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for frac in fractions:\n",
    "    print(f'frac = {frac}')\n",
    "    for i, test_obj in test_sample.iterrows():\n",
    "        print(f'i = {i}')\n",
    "        d, num_iter = utils.find_opt_local_area(obj=test_obj, \n",
    "                                                train_data=train_pos,\n",
    "                                                trainx_min=trainx_min,\n",
    "                                                trainx_max=trainx_max,\n",
    "                                                frac=frac, \n",
    "                                                num_iters=num_iterations\n",
    "                                               )\n",
    "        iterations_list_pos[frac].append(num_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations_list_pos_str = json.dumps(iterations_list_pos)\n",
    "iterations_list_pos_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('iterations_list_pos1.txt', 'w') as f:\n",
    "    f.write(iterations_list_pos_str)\n",
    "    \n",
    "# for key, val in iterations_list_pos.items():\n",
    "#     iterations_list_pos[key] = np.mean(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations_list_pos = pd.DataFrame.from_dict(data=iterations_list_pos, orient='index', columns=['iters'])\n",
    "iterations_list_pos.reset_index(drop=False, inplace=True)\n",
    "iterations_list_pos.rename(columns={'index':'frac'}, inplace=True)\n",
    "iterations_list_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frac in fractions:\n",
    "    print(f'frac = {frac}')\n",
    "    for i, test_obj in test_sample.iterrows():\n",
    "        print(f'i = {i}')\n",
    "        d, num_iter = utils.find_opt_local_area(obj=test_obj, \n",
    "                                                train_data=train_neg,\n",
    "                                                trainx_min=trainx_min,\n",
    "                                                trainx_max=trainx_max,\n",
    "                                                frac=frac, \n",
    "                                                num_iters=num_iterations\n",
    "                                               )\n",
    "        iterations_list_neg[frac].append(num_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"0.01\": [19, 10, 12, 6, 14, 16, 23, 12, 20, 100, 13, 21, 15, 14, 15, 34, 13, 40, 16, 13, 16, 12, 17, 17, 16, 20, 10, 39, 16, 11, 11, 6, 5, 15, 14, 15, 19, 64, 15, 25], \"0.03\": [23, 14, 15, 9, 46, 21, 27, 16, 29, 800, 22, 27, 21, 16, 19, 50, 16, 94, 21, 18, 22, 15, 23, 30, 20, 26, 13, 58, 25, 26, 16, 12, 11, 24, 20, 21, 25, 89, 21, 38], \"0.05\": [25, 16, 16, 15, 81, 24, 30, 18, 33, 1001, 26, 33, 26, 17, 21, 54, 18, 100, 24, 24, 26, 19, 29, 38, 22, 32, 16, 77, 28, 36, 21, 17, 14, 28, 24, 24, 27, 109, 24, 42], \"0.07\": [26, 19, 18, 19, 91, 28, 35, 22, 36, 1001, 32, 36, 32, 20, 24, 58, 20, 202, 27, 29, 32, 22, 33, 42, 25, 35, 18, 91, 33, 42, 24, 23, 19, 32, 32, 25, 31, 117, 30, 65], \"0.09\": [27, 22, 21, 25, 97, 30, 38, 24, 40, 1001, 36, 40, 40, 22, 26, 63, 23, 1001, 30, 35, 41, 24, 36, 48, 26, 37, 20, 99, 36, 47, 27, 29, 23, 34, 35, 26, 33, 127, 37, 77], \"0.11\": [27, 23, 23, 31, 99, 32, 43, 26, 43, 1001, 39, 43, 43, 24, 28, 67, 25, 1001, 32, 41, 54, 26, 39, 52, 29, 38, 23, 108, 40, 61, 29, 35, 27, 37, 39, 28, 35, 132, 47, 84], \"0.13\": [28, 25, 25, 38, 100, 34, 48, 28, 46, 1001, 42, 47, 47, 25, 29, 71, 27, 1001, 34, 47, 64, 28, 43, 55, 30, 41, 25, 114, 44, 81, 32, 42, 33, 39, 44, 33, 36, 140, 59, 89], \"0.15\": [30, 27, 27, 47, 100, 36, 51, 30, 48, 1001, 46, 47, 50, 26, 31, 74, 28, 1001, 35, 54, 74, 30, 46, 58, 32, 43, 27, 120, 46, 93, 35, 51, 42, 41, 46, 40, 38, 144, 68, 92], \"0.17\": [], \"0.19\": [], \"0.21\": [], \"0.23\": [], \"0.25\": [], \"0.27\": [], \"0.29\": []}'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterations_list_neg_str=json.dumps(iterations_list_neg)\n",
    "iterations_list_neg_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('iterations_list_neg.txt', 'w') as f:\n",
    "    f.write(iterations_list_neg_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations_list_neg = pd.DataFrame.from_dict(data=iterations_list_neg, orient='index', columns=['iters'])\n",
    "iterations_list_neg.reset_index(drop=False, inplace=True)\n",
    "iterations_list_neg.rename(columns={'index':'frac'}, inplace=True)\n",
    "iterations_list_neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# iterations_list_pos.to_csv('iterations_list_pos.csv', header=True, index=False)\n",
    "# iterations_list_neg.to_csv('iterations_list_neg.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.lineplot(y='iters', x='frac', data=iterations_list_pos)\n",
    "# sns.lineplot(y='iters', x='frac', data=iterations_list_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Исследование количества генерируемых гипотез в зависимости от набора параметров для нового подхода отбора гипотез**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Сначала проверим построение старого подхода генерации гипотез*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_params = [0.000, 0.001, 0.002, 0.003, 0.004]\n",
    "sample_ratio_params = [0.001, 0.002, 0.004, 0.006, 0.008, 0.01]\n",
    "alpha_params, sample_ratio_params\n",
    "\n",
    "results_pos = {}\n",
    "results_neg = {}\n",
    "pos_hyps = []\n",
    "neg_hyps = []\n",
    "\n",
    "for alpha in alpha_params:\n",
    "    results_pos[alpha] = {}\n",
    "    results_neg[alpha] = {}\n",
    "    \n",
    "for alpha in alpha_params:\n",
    "    for sample_ratio in sample_ratio_params:\n",
    "        results_pos[alpha][sample_ratio] = []\n",
    "        results_neg[alpha][sample_ratio] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'train_pos': train_pos,\n",
    "    'train_neg': train_neg,\n",
    "    'num_iters': 1000,\n",
    "    'hypothesis_criterion': 'contr_class',\n",
    "    'sample_type': 'random',\n",
    "    'trainx_min': trainx_min,\n",
    "    'trainx_max': trainx_max,\n",
    "    'verbose': False,\n",
    "    'n_jobs': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for alpha in alpha_params:\n",
    "    for sample_ratio in sample_ratio_params:\n",
    "        for i, obj in test_sample[:1].iterrows():\n",
    "            print(f\"using params: alpha = {alpha}, sample_ratio = {sample_ratio}, num_iters = {params['num_iters']}\")\n",
    "            pos_hyps = mining_step(test_obj=obj, sample_ratio=sample_ratio,\n",
    "                                   alpha = alpha, mining_type='pos', classes_ratio=classes_ratio_pos,\n",
    "                                   **params\n",
    "                                  )\n",
    "            \n",
    "            pos_hyps_shape = pd.DataFrame(pos_hyps).shape[0] if len(pos_hyps) > 0 else 0\n",
    "            \n",
    "            results_pos[alpha][sample_ratio].append(pos_hyps_shape)\n",
    "            \n",
    "            neg_hyps = mining_step(test_obj=obj, sample_ratio=sample_ratio, \n",
    "                                   alpha = alpha, mining_type='neg', classes_ratio=classes_ratio_neg,\n",
    "                                   **params\n",
    "                                  )\n",
    "            \n",
    "            neg_hyps_shape = pd.DataFrame(neg_hyps).shape[0] if len(neg_hyps) > 0 else 0\n",
    "            \n",
    "            results_neg[alpha][sample_ratio].append(neg_hyps_shape)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Новый подход генерации гипотез*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_params = [1.0, 1.05, 1.1, 1.3, 1.5, 1.7, 2.0]\n",
    "sample_ratio_params = [0.001, 0.002, 0.004, 0.006, 0.008, 0.01]\n",
    "alpha_params, sample_ratio_params\n",
    "\n",
    "results_pos = {}\n",
    "results_neg = {}\n",
    "pos_hyps = []\n",
    "neg_hyps = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'train_pos': train_pos,\n",
    "    'train_neg': train_neg,\n",
    "    'num_iters': 1000,\n",
    "    'hypothesis_criterion': 'both_classes',\n",
    "    'sample_type': 'local',\n",
    "    'trainx_min': trainx_min,\n",
    "    'trainx_max': trainx_max,\n",
    "    'verbose': False,\n",
    "    'n_jobs': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for alpha in alpha_params:\n",
    "    for sample_ratio in sample_ratio_params:\n",
    "        for i, obj in test_sample[:1].iterrows():\n",
    "            print(f\"using params: alpha = {alpha}, sample_ratio = {sample_ratio}, num_iters = {params['num_iters']}\")\n",
    "            pos_hyps = mining_step(test_obj=obj, sample_ratio=sample_ratio,\n",
    "                                   alpha = alpha, mining_type='pos', classes_ratio=classes_ratio_pos,\n",
    "                                   **params\n",
    "                                  )\n",
    "            \n",
    "            pos_hyps_shape = pd.DataFrame(pos_hyps).shape[0] if len(pos_hyps) > 0 else 0\n",
    "            \n",
    "            results_pos[alpha][sample_ratio].append(pos_hyps_shape)\n",
    "            \n",
    "            neg_hyps = mining_step(test_obj=obj, sample_ratio=sample_ratio, \n",
    "                                   alpha = alpha, mining_type='neg', classes_ratio=classes_ratio_neg,\n",
    "                                   **params\n",
    "                                  )\n",
    "            \n",
    "            neg_hyps_shape = pd.DataFrame(neg_hyps).shape[0] if len(neg_hyps) > 0 else 0\n",
    "            \n",
    "            results_neg[alpha][sample_ratio].append(neg_hyps_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оценивание модели, вычисление ROC AUC and Gini coefficient**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Проделаем для стандартного метода, который уже использовался"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Берем множество объеков(тестовое)\n",
    "    Для каждого объекта считаем множество положительных и негативных гипотез\n",
    "    Разница между количеством гипотез будет как раз скором, а знак этой разницы - классом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, test_obg in test_sample[:1].iterrows():\n",
    "    pos_hyps = mining_step(test_obj=test_obj, train_pos=train_pos, train_neg=train_neg, \n",
    "                            num_iters=600,sample_ratio=0.003, alpha = 0.004,\n",
    "                                   hypothesis_criterion='contr_class', sample_type='random',\n",
    "                                   mining_type='pos', trainx_min=trainx_min, trainx_max=trainx_max,\n",
    "                                   classes_ratio=classes_ratio_pos, verbose=False, n_jobs=4\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, test_obg in test_sample[:1].iterrows():\n",
    "    neg_hyps = mining_step(test_obj=test_obj, train_pos=train_pos, train_neg=train_neg, \n",
    "                            num_iters=600,sample_ratio=0.003, alpha = 0.004,\n",
    "                                   hypothesis_criterion='contr_class', sample_type='random',\n",
    "                                   mining_type='neg', trainx_min=trainx_min, trainx_max=trainx_max,\n",
    "                                   classes_ratio=classes_ratio_pos, verbose=False, n_jobs=2\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_hyps = pd.DataFrame(pos_hyps)\n",
    "neg_hyps = pd.DataFrame(neg_hyps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters=600\n",
    "sample_ratio=0.003\n",
    "alpha = 0.004\n",
    "hypothesis_criterion = 'contr_class'\n",
    "sample_type = 'random'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metric_for_obj(test_obj: pd.Series, train_pos: pd.DataFrame, train_neg: pd.DataFrame, \n",
    "                            num_iters, sample_ratio, alpha, hypothesis_criterion, sample_type,\n",
    "                            trainx_min, trainx_max, classes_ratio_pos, classes_ratio_neg, verbose, n_jobs):\n",
    "    \n",
    "    \n",
    "    pos_hyps = mining_step(test_obj=test_obj, train_pos=train_pos, train_neg=train_neg, \n",
    "                           num_iters=num_iters,sample_ratio=sample_ratio, alpha = alpha,\n",
    "                           hypothesis_criterion=hypothesis_criterion, sample_type=sample_type,\n",
    "                           mining_type='pos', trainx_min=trainx_min, trainx_max=trainx_max,\n",
    "                           classes_ratio=classes_ratio_pos, verbose=verbose, n_jobs=n_jobs\n",
    "                        )\n",
    "    pos_hyps = pd.DataFrame(pos_hyps)\n",
    "    \n",
    "    sleep(10)\n",
    "    \n",
    "    neg_hyps = mining_step(test_obj=test_obj, train_pos=train_pos, train_neg=train_neg, \n",
    "                           num_iters=num_iters,sample_ratio=sample_ratio, alpha = alpha,\n",
    "                           hypothesis_criterion=hypothesis_criterion, sample_type=sample_type,\n",
    "                           mining_type='neg', trainx_min=trainx_min, trainx_max=trainx_max,\n",
    "                           classes_ratio=classes_ratio_neg, verbose=verbose, n_jobs=n_jobs\n",
    "                        )\n",
    "    neg_hyps = pd.DataFrame(neg_hyps)\n",
    "    \n",
    "    sleep(5)\n",
    "    \n",
    "    diff = pos_hyps.shape[0] - neg_hyps.shape[0]\n",
    "    label = None\n",
    "    if diff > 0:\n",
    "        label = 1\n",
    "    elif diff < 0:\n",
    "        label = 0\n",
    "    else:\n",
    "        label = -1\n",
    "        \n",
    "    return diff, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(test_sample: pd.DataFrame, train_pos, train_neg,  num_iters, sample_ratio,\n",
    "                   alpha, hypothesis_criterion, sample_type, trainx_min, trainx_max,\n",
    "                   classes_ratio_pos, classes_ratio_neg, verbose=False, n_jobs=2):\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(f'using params: num_iters = {num_iters}, sample_ratio = {sample_ratio}, alpha = {alpha}')\n",
    "    \n",
    "    for i, test_obj in test_sample.iterrows():\n",
    "        diff, label = calculate_metric_for_obj(test_obj=test_obj, train_pos=train_pos, train_neg=train_neg, \n",
    "                                num_iters=num_iters, sample_ratio=sample_ratio, alpha=alpha, \n",
    "                                hypothesis_criterion=hypothesis_criterion, sample_type=sample_type,\n",
    "                                trainx_min=trainx_min, trainx_max=trainx_max,\n",
    "                                 classes_ratio_pos=classes_ratio_pos, classes_ratio_neg=classes_ratio_neg, \n",
    "                                verbose=verbose, n_jobs=n_jobs)\n",
    "        results[i] = (diff, label)\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# results = evaluate_model(test_sample=test_sample[8:9], train_pos=train_pos, train_neg=train_neg, \n",
    "#                          num_iters=1500, sample_ratio=sample_ratio, alpha=alpha, \n",
    "#                                 hypothesis_criterion=hypothesis_criterion, sample_type=sample_type,\n",
    "#                                 trainx_min=trainx_min, trainx_max=trainx_max,\n",
    "#                                  classes_ratio_pos=classes_ratio_pos, classes_ratio_neg=classes_ratio_neg, \n",
    "#                                 verbose=False, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valY[test_sample[8:12].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valY.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# t = hypothesises_to_feat_matrix(pos_hyps=pos_hyps, neg_hyps=neg_hyps, trainX=train_pos.sample(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучение, основанное на признаках, полученных из сгенерированных гипотез**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_binary_repr(num: int, pos_hyps: pd.DataFrame, neg_hyps: pd.DataFrame, indices: list, trainX: pd.DataFrame):\n",
    "    ind = indices[num]\n",
    "    obj = trainX.loc[ind]\n",
    "    pos_features = [f'pos_feat_{feat_num}' for feat_num in pos_hyps.index]\n",
    "    neg_features = [f'neg_feat_{feat_num}' for feat_num in neg_hyps.index]\n",
    "    features = pos_features + neg_features\n",
    "    start_values = np.zeros(shape=(1, len(features)))\n",
    "#     print(start_values.shape)\n",
    "#     print(obj.name)\n",
    "    result = pd.DataFrame(data=start_values,index=[obj.name], columns=features, dtype='int')\n",
    "    ind = obj.name\n",
    "    for pi in range(pos_hyps.shape[0]):\n",
    "        feat_repr = utils.similarity(obj, pos_hyps.iloc[pi])\n",
    "        is_included = pos_hyps.iloc[pi].equals(feat_repr)\n",
    "        if is_included:\n",
    "            result.loc[ind, f'pos_feat_{pi}'] = 1\n",
    "        else:\n",
    "            result.loc[ind, f'pos_feat_{pi}'] = 0\n",
    "        \n",
    "    for pi in range(neg_hyps.shape[0]):\n",
    "        feat_repr = utils.similarity(obj, neg_hyps.iloc[pi])\n",
    "        is_included = neg_hyps.iloc[pi].equals(feat_repr)\n",
    "        if is_included:\n",
    "            result.loc[ind, f'neg_feat_{pi}'] = 1\n",
    "        else:\n",
    "            result.loc[ind, f'neg_feat_{pi}'] = 0\n",
    "                          \n",
    "    return result\n",
    "\n",
    "def transform_to_feature_matrix(pos_hyps: pd.DataFrame, neg_hyps: pd.DataFrame, trainX: pd.DataFrame, n_jobs: int = 4):\n",
    "    \n",
    "    indices = trainX.index\n",
    "    transform_func = partial(to_binary_repr, pos_hyps=pos_hyps, \n",
    "                             neg_hyps=neg_hyps, indices=indices, trainX=trainX)\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=n_jobs) as executor:\n",
    "        obj_features = executor.map(transform_func, range(len(indices)))\n",
    "\n",
    "    features = pd.concat(obj_features)#pd.DataFrame(obj_features)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.07 s, sys: 442 ms, total: 4.51 s\n",
      "Wall time: 3min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gen = transform_to_feature_matrix(pos_hyps=pos_hyps, neg_hyps=neg_hyps, trainX=trainX.sample(512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.18, 1.25, 1.71)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.18, 1.25, 1.71, 2 range 1 4\n",
    "3.95(8), 7.35(16), 29(64), 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "t = to_binary_repr(train_pos.iloc[10], pos_hyps=pos_hyps, neg_hyps=neg_hyps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28125    3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesises_to_feat_matrix(pos_hyps: pd.DataFrame, neg_hyps: pd.DataFrame, trainX: pd.DataFrame):\n",
    "    pos_features = [f'pos_feat_{feat_num}' for feat_num in pos_hyps.index]\n",
    "#     neg_features = [f'neg_feat_{feat_num}' for feat_num in neg_hyps[pi]s.index]\n",
    "    \n",
    "    result = pd.DataFrame(index=trainX.index, columns=pos_features)# + neg_features)\n",
    "    \n",
    "    for i in range(trainX.shape[0]):\n",
    "        for pi in range(pos_hyps.shape[0]):\n",
    "\n",
    "            feat_repr = utils.similarity(obj, pos_hyps.iloc[pi])\n",
    "            is_included = pos_hyps.iloc[pi].equals(feat_repr)\n",
    "            if is_included:\n",
    "                result.loc[i, f'pos_feat_{pi}'] = 1\n",
    "            else:\n",
    "                result.loc[i, f'pos_feat_{pi}'] = 0\n",
    "                \n",
    "        for pi in range(neg_hyps.shape[0]):\n",
    "            feat_repr = utils.similarity(obj, neg_hyps.iloc[pi])\n",
    "            is_included = neg_hyps.iloc[pi].equals(feat_repr)\n",
    "            if is_included:\n",
    "                result.loc[i, f'neg_feat_{pi}'] = 1\n",
    "            else:\n",
    "                result.loc[i, f'neg_feat_{pi}'] = 0\n",
    "    \n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Данный вариант не прокатит, слишком долго\n",
    "### Нужно придумать другой вариант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# result = hypothesises_to_feat_matrix(pos_hyps=pos_hyps, neg_hyps=neg_hyps, trainX=trainX.sample(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучение композиции, основанное на гипотезах, считая их слабыми классификаторами(Бустинг)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_hyps = pd.read_csv('hypothesises/test_pos_hyps.csv')\n",
    "neg_hyps = pd.read_csv('hypothesises/test_neg_hyps.csv')\n",
    "hyps = pd.concat([pos_hyps, neg_hyps])\n",
    "hyps.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyps_boosting(hypothesises: pd.DataFrame, num_iterations):\n",
    "    '''\n",
    "    у гипотез есть индекс, просто число, от 0 до кол-ва гипотез, будем использовать его за id этой гипотезы\n",
    "    '''\n",
    "    \n",
    "    ansamble = []\n",
    "    weights = list(range(1, 11))\n",
    "    weights = [w / 10 for w in weights]\n",
    "    \n",
    "    hyps_indices = list(hypothesises.index)\n",
    "    \n",
    "    ##допустим первую берем случайно(потом переделаем, чтоб брать осознанно)\n",
    "    index = np.random.choice(hyps_indices, replace=False)\n",
    "    h = hypothesises.loc[index]\n",
    "    hyps_indices.remove(index)\n",
    "    ansamble.append(h)\n",
    "    \n",
    "    #тут еще проверяется критерий того, что данный \n",
    "    while not hyps_indices and num_iterations > 0:\n",
    "        \n",
    "        index = np.random.choice(hyps_indices, replace=False)\n",
    "        h = hypothesises.loc[index]\n",
    "        \n",
    "        \n",
    "    \n",
    "    return weights\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyps_boosting(hypothesises=hyps, num_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inds = list(range(13))\n",
    "inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = pd.Series({'feat1':(1,1), 'feat2':(0, 1.4), 'feat3':(3, 4)})\n",
    "# b = pd.Series({'feat1':(1,2), 'feat2':(1, 1.4), 'feat3':(7, 7)})\n",
    "# c = pd.Series({'feat1':(1,1), 'feat2':(0.8, 1.6), 'feat3':(3, 4)})\n",
    "# d = pd.Series({'feat1':(2,3), 'feat2':(0, 1.4), 'feat3':(4, 6)})\n",
    "\n",
    "\n",
    "# print(a.equals(b))\n",
    "# print(a == b)\n",
    "\n",
    "# test1 = pd.Series({'feat1':(1,3), 'feat2':(1, 3.4), 'feat3':(1, 1.9)})\n",
    "# test2 = pd.Series({'feat1':(2,2.2), 'feat2':(2, 2.4), 'feat3':(3, 3.9)})\n",
    "# test3 = pd.Series({'feat1':(3,3), 'feat2':(4, 4.4), 'feat3':(4, 4.9)})\n",
    "\n",
    "g1 = pd.Series({'feat1':(1,1), 'feat2':(1.5, 1.5)})\n",
    "g2 = pd.DataFrame({'feat1':(-1,-1), 'feat2':(0, 0)})\n",
    "g3 = pd.Series({'feat1':(0.5,0.5), 'feat2':(1, 1)})\n",
    "\n",
    "test1 = pd.Series({'feat1':(0.1, 1), 'feat2':(-0.5, 0)})\n",
    "test2 = pd.DataFrame({'feat1':(-0.1, 1), 'feat2':(0.5, 0.8)})\n",
    "tst = pd.Series({'feat1':(-1, 1), 'feat2':(0, 1.5)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,obj in sample.iterrows():\n",
    "#     print(similarity(pd.DataFrame(d).T, obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = pd.DataFrame([g1, g2, g3, test1, test2])\n",
    "# sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feat1        (1, 1)\n",
       "feat2    (1.5, 1.5)\n",
       "dtype: object"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.series.Series, pandas.core.frame.DataFrame)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(d), type(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_included_in_repr(d, sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестовая функция провекри np.random.choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stat(times, data):\n",
    "    print(f\"times = {times}\")\n",
    "    sample = np.random.RandomState().choice(data, replace=False, size=10)\n",
    "    print(f\"sample = {sample}\")\n",
    "    sample_sum = np.sum(sample)\n",
    "    print(f\"sample sum = {sample_sum}\")\n",
    "    return sample_sum\n",
    "\n",
    "def run_computing(n_jobs=2):\n",
    "    data = list(range(100))\n",
    "    \n",
    "    compute_func = partial(compute_stat, data=data)\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=n_jobs) as executor:\n",
    "        results = executor.map(compute_func, list(range(26)))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = run_computing(n_jobs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
